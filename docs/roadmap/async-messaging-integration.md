# Asynchronous Messaging & Multi-Modal Processing Roadmap

## Overview

This roadmap outlines the strategic implementation of asynchronous message queue protocols (RabbitMQ, Kafka, Pub/Sub) to enhance ORBIT's capabilities with real-time inference, multi-modal processing, and event-driven workflows. This adds a powerful asynchronous layer on top of existing synchronous protocols, enabling scalable processing of diverse content types including text, images, audio, video, and documents.

## Current State Analysis

### Synchronous Processing Limitations
- **Blocking Operations**: Current API calls wait for complete processing before returning
- **Single Modal Focus**: Primarily optimized for text-based inference
- **Resource Bottlenecks**: Long-running tasks block other requests
- **Limited Scalability**: Cannot efficiently handle variable processing times
- **No Event-Driven Patterns**: Lacks reactive processing capabilities
- **Batch Processing Gaps**: No native support for bulk operations

### Current Architecture Constraints
```python
# Current synchronous pattern
@app.post("/v1/chat")
async def chat_endpoint(request: ChatRequest):
    # Blocks until complete - problematic for long tasks
    result = await inference_service.process(request)
    return result  # Client waits entire time
```

## Asynchronous Messaging Advantages

### Multi-Modal Processing Benefits
- **Variable Processing Times**: Text (100ms) vs Video (10+ seconds) vs Audio (5+ seconds)
- **Resource Optimization**: Different modalities use different compute resources
- **Parallel Processing**: Multiple content types processed simultaneously
- **Streaming Results**: Progressive results as processing completes
- **Background Processing**: Long-running tasks don't block user interface

### Message Queue Benefits
- **Decoupled Architecture**: Producers and consumers operate independently
- **Fault Tolerance**: Retry mechanisms, dead letter queues, persistence
- **Load Balancing**: Automatic distribution across multiple workers
- **Scalability**: Dynamic scaling based on queue depth
- **Event-Driven**: React to system events and state changes

## Strategic Architecture

### Hybrid Processing Model
```
┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│   ORBIT Client  │────│  ORBIT Gateway   │────│ Sync Response   │
└─────────────────┘    │  (Router)        │    │ (Simple Tasks)  │
                       └──────────────────┘    └─────────────────┘
                                │
                                ▼
                       ┌──────────────────┐
                       │  Message Queue   │
                       │  (RabbitMQ/Kafka)│
                       └──────────────────┘
                                │
                    ┌───────────┼───────────┐
                    ▼           ▼           ▼
            ┌──────────────┐ ┌──────────────┐ ┌──────────────┐
            │   Text       │ │   Image      │ │   Audio/Video│
            │  Processing  │ │  Processing  │ │  Processing  │
            │   Workers    │ │   Workers    │ │   Workers    │
            └──────────────┘ └──────────────┘ └──────────────┘
                    │           │           │
                    └───────────┼───────────┘
                                ▼
                       ┌──────────────────┐
                       │  Result Aggregation│
                       │  & Notification  │
                       └──────────────────┘
                                │
                                ▼
                       ┌──────────────────┐
                       │  WebSocket/SSE   │
                       │  Real-time       │
                       │  Updates         │
                       └──────────────────┘
```

### Processing Flow Decision Tree
```python
# Intelligent routing based on content type and complexity
async def route_request(request: ProcessingRequest):
    if request.estimated_time < 2_seconds and request.content_type == "text":
        # Synchronous processing for quick tasks
        return await sync_processor.process(request)
    else:
        # Asynchronous processing for complex/multi-modal tasks
        job_id = await async_queue.enqueue(request)
        return {"job_id": job_id, "status": "processing", "websocket_url": f"/ws/{job_id}"}
```

## Implementation Roadmap

### Phase 1: Message Queue Foundation (3-4 weeks)

#### 1.1 Multi-Platform Message Queue Support
**Objective**: Implement pluggable message queue backends

**Supported Platforms**:
- **RabbitMQ**: Enterprise-grade AMQP messaging
- **Apache Kafka**: High-throughput streaming platform  
- **Google Cloud Pub/Sub**: Managed serverless messaging
- **AWS SQS/SNS**: Amazon's managed queue services
- **Redis Streams**: Lightweight in-memory messaging
- **Azure Service Bus**: Microsoft's enterprise messaging

**Queue Abstraction Layer**:
```python
# server/messaging/queue_factory.py
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional

class MessageQueueInterface(ABC):
    @abstractmethod
    async def publish(self, topic: str, message: Dict[str, Any], 
                     priority: int = 0, delay: int = 0) -> str:
        """Publish message to queue"""
        pass
    
    @abstractmethod
    async def subscribe(self, topic: str, callback, 
                       consumer_group: str = None) -> None:
        """Subscribe to topic with callback"""
        pass
    
    @abstractmethod
    async def acknowledge(self, message_id: str) -> None:
        """Acknowledge message processing"""
        pass

class RabbitMQAdapter(MessageQueueInterface):
    def __init__(self, connection_url: str):
        self.connection = aio_pika.connect_robust(connection_url)
        self.channel = None
    
    async def publish(self, topic: str, message: Dict[str, Any], 
                     priority: int = 0, delay: int = 0) -> str:
        message_body = aio_pika.Message(
            json.dumps(message).encode(),
            priority=priority,
            delivery_mode=aio_pika.DeliveryMode.PERSISTENT
        )
        
        if delay > 0:
            # Use delayed exchange for deferred processing
            await self.channel.default_exchange.publish(
                message_body, 
                routing_key=f"{topic}.delayed.{delay}"
            )
        else:
            await self.channel.default_exchange.publish(
                message_body, 
                routing_key=topic
            )
        
        return message_body.message_id

class KafkaAdapter(MessageQueueInterface):
    def __init__(self, bootstrap_servers: str):
        self.producer = AIOKafkaProducer(
            bootstrap_servers=bootstrap_servers,
            value_serializer=lambda x: json.dumps(x).encode()
        )
        self.consumer = None
    
    async def publish(self, topic: str, message: Dict[str, Any], 
                     priority: int = 0, delay: int = 0) -> str:
        headers = [("priority", str(priority).encode())]
        if delay > 0:
            headers.append(("delay_until", str(time.time() + delay).encode()))
        
        record_metadata = await self.producer.send(
            topic, 
            value=message,
            headers=headers
        )
        return f"{record_metadata.topic}-{record_metadata.partition}-{record_metadata.offset}"
```

#### 1.2 Job Management System
**Objective**: Comprehensive async job tracking and management

```python
# server/messaging/job_manager.py
from enum import Enum
from dataclasses import dataclass
from typing import Optional, Dict, Any
import uuid

class JobStatus(Enum):
    QUEUED = "queued"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

@dataclass
class JobResult:
    job_id: str
    status: JobStatus
    progress: float
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    created_at: datetime
    updated_at: datetime
    estimated_completion: Optional[datetime] = None

class AsyncJobManager:
    def __init__(self, queue_backend: MessageQueueInterface, 
                 storage_backend: 'JobStorage'):
        self.queue = queue_backend
        self.storage = storage_backend
        self.active_jobs: Dict[str, JobResult] = {}
    
    async def submit_job(self, job_type: str, payload: Dict[str, Any], 
                        priority: int = 0, user_id: str = None) -> str:
        """Submit job for asynchronous processing"""
        job_id = str(uuid.uuid4())
        
        job = JobResult(
            job_id=job_id,
            status=JobStatus.QUEUED,
            progress=0.0,
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow()
        )
        
        # Store job metadata
        await self.storage.save_job(job)
        
        # Queue the job
        message = {
            "job_id": job_id,
            "job_type": job_type,
            "payload": payload,
            "user_id": user_id,
            "priority": priority,
            "submitted_at": job.created_at.isoformat()
        }
        
        topic = f"orbit.processing.{job_type}"
        await self.queue.publish(topic, message, priority=priority)
        
        return job_id
    
    async def get_job_status(self, job_id: str) -> Optional[JobResult]:
        """Get current job status and results"""
        return await self.storage.get_job(job_id)
    
    async def update_job_progress(self, job_id: str, progress: float, 
                                 status: JobStatus = None, 
                                 partial_result: Dict[str, Any] = None):
        """Update job progress and status"""
        job = await self.storage.get_job(job_id)
        if not job:
            return
        
        job.progress = progress
        job.updated_at = datetime.utcnow()
        
        if status:
            job.status = status
        
        if partial_result:
            if not job.result:
                job.result = {}
            job.result.update(partial_result)
        
        await self.storage.save_job(job)
        
        # Notify subscribers
        await self.notify_job_update(job)
```

#### 1.3 Multi-Modal Content Detection
**Objective**: Automatic content type detection and routing

```python
# server/messaging/content_classifier.py
import magic
from typing import Tuple, Dict, Any
import aiofiles

class ContentTypeClassifier:
    """Intelligent content type detection for routing decisions"""
    
    def __init__(self):
        self.mime_type_mapping = {
            # Text content
            'text/plain': ('text', 'text_processor'),
            'application/json': ('text', 'text_processor'),
            'text/markdown': ('text', 'text_processor'),
            
            # Image content
            'image/jpeg': ('image', 'image_processor'),
            'image/png': ('image', 'image_processor'),
            'image/webp': ('image', 'image_processor'),
            'image/gif': ('image', 'image_processor'),
            
            # Audio content
            'audio/wav': ('audio', 'audio_processor'),
            'audio/mp3': ('audio', 'audio_processor'),
            'audio/ogg': ('audio', 'audio_processor'),
            'audio/flac': ('audio', 'audio_processor'),
            
            # Video content
            'video/mp4': ('video', 'video_processor'),
            'video/avi': ('video', 'video_processor'),
            'video/mov': ('video', 'video_processor'),
            'video/webm': ('video', 'video_processor'),
            
            # Document content
            'application/pdf': ('document', 'document_processor'),
            'application/msword': ('document', 'document_processor'),
            'application/vnd.openxmlformats-officedocument.wordprocessingml.document': ('document', 'document_processor'),
        }
        
        self.processing_estimates = {
            'text': 0.1,      # 100ms average
            'image': 2.0,     # 2 seconds average
            'audio': 5.0,     # 5 seconds average
            'video': 15.0,    # 15 seconds average
            'document': 3.0,  # 3 seconds average
        }
    
    async def classify_content(self, content: bytes, 
                              filename: str = None) -> Tuple[str, str, float]:
        """
        Classify content and return (content_type, processor, estimated_time)
        """
        # Detect MIME type
        mime_type = magic.from_buffer(content, mime=True)
        
        # Get content type and processor
        if mime_type in self.mime_type_mapping:
            content_type, processor = self.mime_type_mapping[mime_type]
        else:
            # Default to text for unknown types
            content_type, processor = 'text', 'text_processor'
        
        # Estimate processing time based on content size and type
        base_time = self.processing_estimates[content_type]
        size_factor = len(content) / (1024 * 1024)  # Size in MB
        estimated_time = base_time * (1 + size_factor * 0.1)
        
        return content_type, processor, estimated_time
    
    def should_process_async(self, content_type: str, estimated_time: float, 
                           force_sync: bool = False) -> bool:
        """Determine if content should be processed asynchronously"""
        if force_sync:
            return False
        
        # Use async for complex content or long processing times
        if content_type in ['video', 'audio'] or estimated_time > 2.0:
            return True
        
        return False
```

### Phase 2: Multi-Modal Processors

#### 2.1 Specialized Worker Pools
**Objective**: Dedicated processing workers for each content type

```python
# server/workers/base_worker.py
from abc import ABC, abstractmethod
from typing import Dict, Any, AsyncGenerator
import asyncio

class BaseWorker(ABC):
    def __init__(self, worker_id: str, config: Dict[str, Any]):
        self.worker_id = worker_id
        self.config = config
        self.is_busy = False
        self.current_job = None
    
    @abstractmethod
    async def process(self, job_data: Dict[str, Any]) -> AsyncGenerator[Dict[str, Any], None]:
        """Process job and yield progress updates"""
        pass
    
    async def health_check(self) -> Dict[str, Any]:
        """Worker health status"""
        return {
            "worker_id": self.worker_id,
            "status": "busy" if self.is_busy else "idle",
            "current_job": self.current_job,
            "memory_usage": self.get_memory_usage(),
            "cpu_usage": self.get_cpu_usage()
        }

# server/workers/text_worker.py
class TextProcessingWorker(BaseWorker):
    """Optimized for text-based inference tasks"""
    
    async def process(self, job_data: Dict[str, Any]) -> AsyncGenerator[Dict[str, Any], None]:
        self.is_busy = True
        self.current_job = job_data["job_id"]
        
        try:
            content = job_data["payload"]["content"]
            processing_type = job_data["payload"].get("type", "chat")
            
            yield {"progress": 0.1, "status": "started", "message": "Initializing text processing"}
            
            if processing_type == "chat":
                result = await self.process_chat(content)
            elif processing_type == "embedding":
                result = await self.process_embedding(content)
            elif processing_type == "summary":
                result = await self.process_summary(content)
            else:
                raise ValueError(f"Unknown text processing type: {processing_type}")
            
            yield {"progress": 1.0, "status": "completed", "result": result}
            
        except Exception as e:
            yield {"progress": 1.0, "status": "failed", "error": str(e)}
        finally:
            self.is_busy = False
            self.current_job = None

# server/workers/image_worker.py
class ImageProcessingWorker(BaseWorker):
    """Optimized for image analysis and generation tasks"""
    
    def __init__(self, worker_id: str, config: Dict[str, Any]):
        super().__init__(worker_id, config)
        self.vision_models = self.load_vision_models()
    
    async def process(self, job_data: Dict[str, Any]) -> AsyncGenerator[Dict[str, Any], None]:
        self.is_busy = True
        self.current_job = job_data["job_id"]
        
        try:
            image_data = job_data["payload"]["content"]
            processing_type = job_data["payload"].get("type", "analyze")
            
            yield {"progress": 0.1, "status": "started", "message": "Loading image"}
            
            # Decode image
            image = await self.decode_image(image_data)
            yield {"progress": 0.3, "status": "processing", "message": "Image loaded, analyzing content"}
            
            if processing_type == "analyze":
                result = await self.analyze_image(image)
            elif processing_type == "caption":
                result = await self.generate_caption(image)
            elif processing_type == "ocr":
                result = await self.extract_text(image)
            elif processing_type == "object_detection":
                result = await self.detect_objects(image)
            else:
                raise ValueError(f"Unknown image processing type: {processing_type}")
            
            yield {"progress": 1.0, "status": "completed", "result": result}
            
        except Exception as e:
            yield {"progress": 1.0, "status": "failed", "error": str(e)}
        finally:
            self.is_busy = False
            self.current_job = None

# server/workers/audio_worker.py  
class AudioProcessingWorker(BaseWorker):
    """Optimized for audio transcription and analysis"""
    
    async def process(self, job_data: Dict[str, Any]) -> AsyncGenerator[Dict[str, Any], None]:
        self.is_busy = True
        self.current_job = job_data["job_id"]
        
        try:
            audio_data = job_data["payload"]["content"]
            processing_type = job_data["payload"].get("type", "transcribe")
            
            yield {"progress": 0.1, "status": "started", "message": "Processing audio"}
            
            # Preprocess audio
            audio = await self.preprocess_audio(audio_data)
            yield {"progress": 0.3, "status": "processing", "message": "Audio preprocessed, transcribing"}
            
            if processing_type == "transcribe":
                result = await self.transcribe_audio(audio)
            elif processing_type == "speaker_identification":
                result = await self.identify_speakers(audio)
            elif processing_type == "sentiment_analysis":
                result = await self.analyze_audio_sentiment(audio)
            elif processing_type == "music_analysis":
                result = await self.analyze_music(audio)
            else:
                raise ValueError(f"Unknown audio processing type: {processing_type}")
            
            yield {"progress": 1.0, "status": "completed", "result": result}
            
        except Exception as e:
            yield {"progress": 1.0, "status": "failed", "error": str(e)}
        finally:
            self.is_busy = False
            self.current_job = None

# server/workers/video_worker.py
class VideoProcessingWorker(BaseWorker):
    """Optimized for video analysis and processing"""
    
    async def process(self, job_data: Dict[str, Any]) -> AsyncGenerator[Dict[str, Any], None]:
        self.is_busy = True
        self.current_job = job_data["job_id"]
        
        try:
            video_data = job_data["payload"]["content"]
            processing_type = job_data["payload"].get("type", "analyze")
            
            yield {"progress": 0.05, "status": "started", "message": "Loading video"}
            
            # Extract video metadata
            metadata = await self.extract_video_metadata(video_data)
            yield {"progress": 0.1, "status": "processing", "message": "Video loaded, processing frames"}
            
            if processing_type == "analyze":
                result = await self.analyze_video_content(video_data, metadata)
            elif processing_type == "transcribe":
                result = await self.transcribe_video_audio(video_data)
            elif processing_type == "scene_detection":
                result = await self.detect_scenes(video_data)
            elif processing_type == "object_tracking":
                result = await self.track_objects(video_data)
            else:
                raise ValueError(f"Unknown video processing type: {processing_type}")
            
            yield {"progress": 1.0, "status": "completed", "result": result}
            
        except Exception as e:
            yield {"progress": 1.0, "status": "failed", "error": str(e)}
        finally:
            self.is_busy = False
            self.current_job = None
```

#### 2.2 Dynamic Worker Scaling
**Objective**: Auto-scale workers based on queue depth and content type

```python
# server/messaging/worker_manager.py
class WorkerManager:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.worker_pools = {
            'text': [],
            'image': [],
            'audio': [],
            'video': [],
            'document': []
        }
        self.scaling_policies = self.load_scaling_policies()
    
    async def scale_workers(self, content_type: str, queue_depth: int):
        """Dynamically scale workers based on demand"""
        policy = self.scaling_policies[content_type]
        current_workers = len(self.worker_pools[content_type])
        
        # Calculate desired worker count
        if queue_depth > policy['scale_up_threshold']:
            desired_workers = min(
                current_workers + policy['scale_up_increment'],
                policy['max_workers']
            )
        elif queue_depth < policy['scale_down_threshold']:
            desired_workers = max(
                current_workers - policy['scale_down_increment'],
                policy['min_workers']
            )
        else:
            desired_workers = current_workers
        
        # Scale up
        if desired_workers > current_workers:
            for i in range(desired_workers - current_workers):
                worker = await self.create_worker(content_type)
                self.worker_pools[content_type].append(worker)
                await worker.start()
        
        # Scale down
        elif desired_workers < current_workers:
            for i in range(current_workers - desired_workers):
                worker = self.worker_pools[content_type].pop()
                await worker.stop()
    
    def load_scaling_policies(self) -> Dict[str, Dict[str, Any]]:
        return {
            'text': {
                'min_workers': 2,
                'max_workers': 10,
                'scale_up_threshold': 5,
                'scale_down_threshold': 1,
                'scale_up_increment': 2,
                'scale_down_increment': 1
            },
            'image': {
                'min_workers': 1,
                'max_workers': 5,
                'scale_up_threshold': 3,
                'scale_down_threshold': 0,
                'scale_up_increment': 1,
                'scale_down_increment': 1
            },
            'audio': {
                'min_workers': 1,
                'max_workers': 3,
                'scale_up_threshold': 2,
                'scale_down_threshold': 0,
                'scale_up_increment': 1,
                'scale_down_increment': 1
            },
            'video': {
                'min_workers': 1,
                'max_workers': 2,
                'scale_up_threshold': 1,
                'scale_down_threshold': 0,
                'scale_up_increment': 1,
                'scale_down_increment': 1
            }
        }
```

### Phase 3: Real-Time Communication

#### 3.1 WebSocket Integration for Live Updates
**Objective**: Real-time job progress and result streaming

```python
# server/realtime/websocket_manager.py
from fastapi import WebSocket, WebSocketDisconnect
from typing import Dict, Set
import json

class WebSocketManager:
    def __init__(self):
        self.active_connections: Dict[str, Set[WebSocket]] = {}
        self.connection_jobs: Dict[WebSocket, str] = {}
    
    async def connect(self, websocket: WebSocket, job_id: str):
        """Connect client to job updates"""
        await websocket.accept()
        
        if job_id not in self.active_connections:
            self.active_connections[job_id] = set()
        
        self.active_connections[job_id].add(websocket)
        self.connection_jobs[websocket] = job_id
    
    async def disconnect(self, websocket: WebSocket):
        """Disconnect client"""
        if websocket in self.connection_jobs:
            job_id = self.connection_jobs[websocket]
            self.active_connections[job_id].discard(websocket)
            del self.connection_jobs[websocket]
            
            # Clean up empty job connections
            if not self.active_connections[job_id]:
                del self.active_connections[job_id]
    
    async def send_job_update(self, job_id: str, update: Dict[str, Any]):
        """Send update to all clients watching a job"""
        if job_id in self.active_connections:
            message = json.dumps(update)
            disconnected = set()
            
            for websocket in self.active_connections[job_id]:
                try:
                    await websocket.send_text(message)
                except:
                    disconnected.add(websocket)
            
            # Clean up disconnected clients
            for websocket in disconnected:
                self.active_connections[job_id].discard(websocket)
                if websocket in self.connection_jobs:
                    del self.connection_jobs[websocket]

# WebSocket endpoint for real-time updates
@app.websocket("/ws/job/{job_id}")
async def websocket_job_endpoint(websocket: WebSocket, job_id: str):
    await websocket_manager.connect(websocket, job_id)
    try:
        while True:
            # Keep connection alive and handle client messages
            data = await websocket.receive_text()
            message = json.loads(data)
            
            if message.get("type") == "ping":
                await websocket.send_text(json.dumps({"type": "pong"}))
            elif message.get("type") == "cancel_job":
                await job_manager.cancel_job(job_id)
                
    except WebSocketDisconnect:
        await websocket_manager.disconnect(websocket)
```

#### 3.2 Server-Sent Events (SSE) Alternative
```python
# server/realtime/sse_manager.py
from fastapi import Request
from fastapi.responses import StreamingResponse
import asyncio
import json

class SSEManager:
    def __init__(self):
        self.job_streams: Dict[str, asyncio.Queue] = {}
    
    async def create_job_stream(self, job_id: str):
        """Create SSE stream for job updates"""
        self.job_streams[job_id] = asyncio.Queue()
    
    async def send_job_update(self, job_id: str, update: Dict[str, Any]):
        """Send update to SSE stream"""
        if job_id in self.job_streams:
            await self.job_streams[job_id].put(update)
    
    async def job_event_stream(self, job_id: str):
        """Generate SSE events for job"""
        if job_id not in self.job_streams:
            await self.create_job_stream(job_id)
        
        queue = self.job_streams[job_id]
        
        try:
            while True:
                # Wait for update with timeout
                try:
                    update = await asyncio.wait_for(queue.get(), timeout=30.0)
                    yield f"data: {json.dumps(update)}\n\n"
                    
                    # End stream if job is complete
                    if update.get("status") in ["completed", "failed", "cancelled"]:
                        break
                        
                except asyncio.TimeoutError:
                    # Send keepalive
                    yield f"data: {json.dumps({'type': 'keepalive'})}\n\n"
                    
        finally:
            # Clean up stream
            if job_id in self.job_streams:
                del self.job_streams[job_id]

@app.get("/sse/job/{job_id}")
async def job_sse_endpoint(job_id: str):
    """Server-Sent Events endpoint for job updates"""
    return StreamingResponse(
        sse_manager.job_event_stream(job_id),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )
```

### Phase 4: Advanced Features

#### 4.1 Event-Driven Workflows
**Objective**: Chain multi-modal processing tasks based on events

```python
# server/workflows/event_driven_workflows.py
from typing import List, Dict, Any, Callable
from dataclasses import dataclass

@dataclass
class WorkflowStep:
    step_id: str
    processor_type: str
    configuration: Dict[str, Any]
    conditions: List[str] = None  # Conditions to execute this step
    next_steps: List[str] = None  # Next steps to execute

class EventDrivenWorkflow:
    def __init__(self, workflow_id: str, steps: List[WorkflowStep]):
        self.workflow_id = workflow_id
        self.steps = {step.step_id: step for step in steps}
        self.event_handlers: Dict[str, Callable] = {}
    
    async def execute(self, initial_data: Dict[str, Any]) -> str:
        """Execute workflow starting from initial step"""
        workflow_job_id = str(uuid.uuid4())
        
        # Start workflow execution
        await self.execute_step("start", initial_data, workflow_job_id)
        
        return workflow_job_id
    
    async def execute_step(self, step_id: str, data: Dict[str, Any], 
                          workflow_job_id: str):
        """Execute a single workflow step"""
        if step_id not in self.steps:
            return
        
        step = self.steps[step_id]
        
        # Check conditions
        if step.conditions and not self.check_conditions(step.conditions, data):
            return
        
        # Submit job for this step
        job_id = await job_manager.submit_job(
            job_type=step.processor_type,
            payload={
                **data,
                "workflow_id": workflow_job_id,
                "step_id": step_id,
                "configuration": step.configuration
            }
        )
        
        # Set up completion handler
        await self.setup_completion_handler(job_id, step, workflow_job_id, data)
    
    async def setup_completion_handler(self, job_id: str, step: WorkflowStep,
                                     workflow_job_id: str, original_data: Dict[str, Any]):
        """Handle step completion and trigger next steps"""
        # This would be called when the job completes
        result = await job_manager.get_job_status(job_id)
        
        if result.status == JobStatus.COMPLETED:
            # Merge results with original data
            enhanced_data = {**original_data, **result.result}
            
            # Execute next steps
            if step.next_steps:
                for next_step_id in step.next_steps:
                    await self.execute_step(next_step_id, enhanced_data, workflow_job_id)

# Example: Multi-modal content analysis workflow
video_analysis_workflow = EventDrivenWorkflow("video_analysis", [
    WorkflowStep(
        step_id="extract_audio",
        processor_type="video",
        configuration={"type": "extract_audio"}
    ),
    WorkflowStep(
        step_id="transcribe_audio", 
        processor_type="audio",
        configuration={"type": "transcribe"},
        conditions=["audio_extracted"],
        next_steps=["analyze_transcript"]
    ),
    WorkflowStep(
        step_id="extract_frames",
        processor_type="video", 
        configuration={"type": "extract_frames", "interval": 1}
    ),
    WorkflowStep(
        step_id="analyze_frames",
        processor_type="image",
        configuration={"type": "analyze", "batch": True},
        conditions=["frames_extracted"],
        next_steps=["combine_analysis"]
    ),
    WorkflowStep(
        step_id="combine_analysis",
        processor_type="text",
        configuration={"type": "summarize_multimodal"},
        conditions=["transcript_analyzed", "frames_analyzed"]
    )
])
```

#### 4.2 Priority Queue Management
```python
# server/messaging/priority_queue.py
from heapq import heappush, heappop
from typing import Dict, Any, Tuple
import time

class PriorityQueueManager:
    def __init__(self):
        self.queues: Dict[str, List[Tuple[int, float, Dict[str, Any]]]] = {}
        self.priority_levels = {
            'urgent': 0,      # Real-time requests
            'high': 1,        # Interactive requests  
            'normal': 2,      # Standard requests
            'low': 3,         # Batch processing
            'batch': 4        # Background processing
        }
    
    async def enqueue(self, queue_name: str, job: Dict[str, Any], 
                     priority: str = 'normal'):
        """Add job to priority queue"""
        if queue_name not in self.queues:
            self.queues[queue_name] = []
        
        priority_num = self.priority_levels.get(priority, 2)
        timestamp = time.time()
        
        # Higher priority and older timestamps come first
        heappush(self.queues[queue_name], (priority_num, timestamp, job))
    
    async def dequeue(self, queue_name: str) -> Dict[str, Any]:
        """Get highest priority job from queue"""
        if queue_name not in self.queues or not self.queues[queue_name]:
            return None
        
        priority_num, timestamp, job = heappop(self.queues[queue_name])
        return job
    
    async def get_queue_stats(self, queue_name: str) -> Dict[str, Any]:
        """Get queue statistics"""
        if queue_name not in self.queues:
            return {"total": 0, "by_priority": {}}
        
        queue = self.queues[queue_name]
        stats = {"total": len(queue), "by_priority": {}}
        
        for priority_name, priority_num in self.priority_levels.items():
            count = sum(1 for p, t, j in queue if p == priority_num)
            stats["by_priority"][priority_name] = count
        
        return stats
```

## Configuration & Integration

### Message Queue Configuration
```yaml
# config.yaml additions
messaging:
  enabled: true
  backend: "rabbitmq"  # rabbitmq, kafka, pubsub, sqs, redis
  
  # RabbitMQ configuration
  rabbitmq:
    url: "amqp://localhost:5672"
    exchange: "orbit.processing"
    queues:
      text: "orbit.text"
      image: "orbit.image"
      audio: "orbit.audio"
      video: "orbit.video"
      document: "orbit.document"
    
  # Kafka configuration  
  kafka:
    bootstrap_servers: "localhost:9092"
    topics:
      text: "orbit-text-processing"
      image: "orbit-image-processing"
      audio: "orbit-audio-processing"
      video: "orbit-video-processing"
    
  # Google Cloud Pub/Sub
  pubsub:
    project_id: "${GOOGLE_CLOUD_PROJECT}"
    topics:
      text: "orbit-text-processing"
      image: "orbit-image-processing"
      audio: "orbit-audio-processing"
      video: "orbit-video-processing"

# Processing configuration
async_processing:
  enabled: true
  sync_threshold: 2.0  # Seconds - above this, use async
  
  # Worker configuration
  workers:
    text:
      min: 2
      max: 10
      timeout: 30
    image:
      min: 1 
      max: 5
      timeout: 60
    audio:
      min: 1
      max: 3
      timeout: 120
    video:
      min: 1
      max: 2
      timeout: 300
  
  # Job storage
  job_storage:
    backend: "mongodb"  # mongodb, redis, postgresql
    retention_days: 7
    cleanup_interval: 3600

# Real-time communication
realtime:
  websockets:
    enabled: true
    max_connections: 1000
    heartbeat_interval: 30
  
  sse:
    enabled: true
    max_connections: 500
    keepalive_interval: 30
```

### Enhanced API Endpoints
```python
# New async processing endpoints
@app.post("/v1/process/async")
async def submit_async_job(request: AsyncProcessingRequest):
    """Submit job for asynchronous processing"""
    content_type, processor, estimated_time = await content_classifier.classify_content(
        request.content, request.filename
    )
    
    if not content_classifier.should_process_async(content_type, estimated_time, request.force_sync):
        # Process synchronously for quick tasks
        return await process_sync(request)
    
    # Submit for async processing
    job_id = await job_manager.submit_job(
        job_type=processor,
        payload=request.dict(),
        priority=request.priority,
        user_id=request.user_id
    )
    
    return {
        "job_id": job_id,
        "status": "queued",
        "estimated_time": estimated_time,
        "websocket_url": f"/ws/job/{job_id}",
        "sse_url": f"/sse/job/{job_id}",
        "polling_url": f"/v1/job/{job_id}/status"
    }

@app.get("/v1/job/{job_id}/status")
async def get_job_status(job_id: str):
    """Get job status and results"""
    return await job_manager.get_job_status(job_id)

@app.post("/v1/job/{job_id}/cancel")
async def cancel_job(job_id: str):
    """Cancel a running job"""
    return await job_manager.cancel_job(job_id)

@app.post("/v1/workflow/execute")
async def execute_workflow(request: WorkflowExecutionRequest):
    """Execute multi-step workflow"""
    workflow = await workflow_manager.get_workflow(request.workflow_id)
    execution_id = await workflow.execute(request.initial_data)
    
    return {
        "execution_id": execution_id,
        "status": "started",
        "websocket_url": f"/ws/workflow/{execution_id}"
    }
```

## Performance Benefits

### Scalability Improvements
| Metric | Current Sync | With Async Messaging |
|--------|-------------|----------------------|
| **Concurrent Requests** | 1000 | 10,000+ |
| **Multi-Modal Support** | Limited | Full support |
| **Queue Tolerance** | None | Millions of jobs |
| **Processing Flexibility** | Fixed | Dynamic scaling |
| **Resource Utilization** | 60-70% | 85-95% |

### Real-World Use Cases
1. **Batch Document Processing**: Upload 1000 PDFs, get results as they complete
2. **Video Analysis Pipeline**: Extract audio → transcribe → analyze frames → generate summary  
3. **Real-Time Content Moderation**: Process images/videos without blocking API responses
4. **Multi-Language Translation**: Chain text extraction → translation → summarization
5. **Research Data Processing**: Handle large datasets with progress tracking

## Integration with Existing Roadmap

### Synergies with Other Components
- **🔄 Concurrency & Performance**: Message queues handle load spikes better than thread pools
- **🛡️ LLM Guard Integration**: Async security scanning for complex content
- **⚡ Workflow Adapters**: Event-driven multi-step processing workflows
- **📊 Enterprise Features**: Advanced job analytics and resource monitoring

### Migration Strategy
```python
# Gradual migration approach
class HybridProcessingService:
    async def process_request(self, request: ProcessingRequest):
        # Intelligent routing between sync and async
        if self.should_use_async(request):
            return await self.process_async(request)
        else:
            return await self.process_sync(request)
    
    def should_use_async(self, request: ProcessingRequest) -> bool:
        return (
            request.content_type in ['video', 'audio'] or
            request.estimated_time > 2.0 or
            request.prefer_async or
            self.is_system_overloaded()
        )
```

## Expected Timeline

### Phase 1: Foundation
- ✅ Message queue abstraction layer
- ✅ Job management system
- ✅ Content type classification
- ✅ Basic worker implementation

### Phase 2: Multi-Modal
- ✅ Specialized worker pools
- ✅ Dynamic scaling
- ✅ Multi-modal processing pipelines
- ✅ Performance optimization

### Phase 3: Real-Time
- ✅ WebSocket integration
- ✅ Server-Sent Events
- ✅ Progress tracking
- ✅ Client SDKs

### Phase 4: Advanced
- ✅ Event-driven workflows
- ✅ Priority queue management
- ✅ Advanced analytics
- ✅ Enterprise integration

## Success Criteria

### Performance Targets
- ✅ 10,000+ concurrent async jobs
- ✅ <100ms job submission time
- ✅ 99.9% message delivery reliability
- ✅ Real-time progress updates (<1s latency)

### Multi-Modal Capabilities
- ✅ Support for 5+ content types (text, image, audio, video, document)
- ✅ Intelligent content routing based on type and complexity
- ✅ Multi-step workflow execution
- ✅ Cross-modal result correlation

### Enterprise Features
- ✅ Priority-based job processing
- ✅ Comprehensive job analytics
- ✅ Auto-scaling based on demand
- ✅ Fault tolerance and error recovery