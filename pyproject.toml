[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "orbit"
version = "0.1.0"
description = "ORBIT - Open Retrieval-Based Inference Toolkit"
readme = "README.md"
requires-python = ">=3.9"
license = "MIT"
authors = [
    { name = "Orbit Team" }
]
dependencies = [
    "ollama==0.4.7",
    "llama-cpp-python==0.3.8",
    "chromadb>=1.0.7",
    "langchain-ollama>=0.2.3",
    "langchain-community>=0.0.10",
    "huggingface-hub==0.30.2",
    "PyYAML>=6.0.1",
    "tqdm>=4.66.2",
    "fastapi>=0.111.0",
    "uvicorn==0.28.0",
    "python-dotenv==1.0.1",
    "requests==2.31.0",
    "pydantic>=2.10.0",
    "aiohttp>=3.11.1",
    "elasticsearch==9.0.0",
    "python-json-logger>=2.0.7",
    "python-multipart>=0.0.14",
    "pytest>=8.3.5",
    "motor>=3.7.0",
    "pymongo>=4.12.0",
    "aiodns>=3.2.0",
    "langdetect>=1.0.9",
    "google-generativeai==0.8.5",
    "openai==1.76",
    "deepseek==1.0.0",
    "cohere==5.15.0",
    "groq==0.23.1",
    "mistralai==1.7.0",
    "anthropic==0.50.0",
    "pycountry>=24.6.1",
    "safetensors==0.5.3",
    "regex==2024.11.6",
    "sseclient-py==1.8.0"
]

[project.optional-dependencies]
dev = [
    "pytest>=8.3.5",
    "black",
    "isort",
    "mypy",
    "ruff"
]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
addopts = "-v --cov=orbit"

[tool.black]
line-length = 100
target-version = ["py39"]

[tool.isort]
profile = "black"
line_length = 100
multi_line_output = 3

[tool.mypy]
python_version = "3.9"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true

[tool.ruff]
line-length = 100
target-version = "py39"
select = ["E", "F", "B", "I"]
ignore = []

[tool.orbit]
general = { port = 3000, verbose = true, inference_provider = "ollama", adapter = "adapters.chroma.qa", mcp_protocol = false }
general.https = { enabled = false, port = 3443, cert_file = "./cert.pem", key_file = "./key.pem" }

messages = { 
    no_results_response = "I'm sorry, but I don't have any specific information about that topic in my knowledge base.",
    collection_not_found = "I couldn't find the requested collection. Please make sure the collection exists before querying it."
}

embedding = { provider = "ollama", enabled = true, fail_on_error = false }

api_keys = { header_name = "X-API-Key", prefix = "orbit_", require_for_health = true }

logging = { 
    level = "INFO",
    capture_warnings = true,
    propagate = false
}
logging.file = { 
    enabled = true,
    directory = "logs",
    filename = "server.log",
    max_size_mb = 10,
    backup_count = 30,
    rotation = "midnight",
    format = "json"
}
logging.console = { enabled = true, format = "text" }

internal_services.elasticsearch = { 
    enabled = false,
    node = "https://localhost:9200",
    index = "orbit",
    api_key = "${INTERNAL_SERVICES_ELASTICSEARCH_API_KEY}"
}

internal_services.mongodb = {
    host = "localhost",
    port = 27017,
    database = "orbit",
    apikey_collection = "api_keys",
    username = "${INTERNAL_SERVICES_MONGODB_USERNAME}",
    password = "${INTERNAL_SERVICES_MONGODB_PASSWORD}"
}

embeddings.llama_cpp = {
    model_path = "gguf/nomic-embed-text-v1.5-Q4_0.gguf",
    model = "nomic-embed-text-v1.5-Q4_0",
    n_ctx = 1024,
    n_threads = 4,
    n_gpu_layers = -1,
    main_gpu = 0,
    tensor_split = null,
    batch_size = 8,
    dimensions = 768,
    embed_type = "llama_embedding"
}

embeddings.ollama = {
    base_url = "http://localhost:11434",
    model = "nomic-embed-text",
    dimensions = 768
}

embeddings.jina = {
    api_key = "${JINA_API_KEY}",
    base_url = "https://api.jina.ai/v1",
    model = "jina-embeddings-v3",
    task = "text-matching",
    dimensions = 1024,
    batch_size = 10
}

embeddings.openai = {
    api_key = "${OPENAI_API_KEY}",
    model = "text-embedding-3-large",
    dimensions = 1024,
    batch_size = 10
}

embeddings.cohere = {
    api_key = "${COHERE_API_KEY}",
    model = "embed-english-v3.0",
    input_type = "search_document",
    dimensions = 1024,
    batch_size = 32,
    truncate = "NONE",
    embedding_types = ["float"]
}

embeddings.mistral = {
    api_key = "${MISTRAL_API_KEY}",
    api_base = "https://api.mistral.ai/v1",
    model = "mistral-embed",
    dimensions = 1024
}

[[tool.orbit.adapters]]
type = "retriever"
datasource = "sqlite"
adapter = "qa"
implementation = "retrievers.implementations.sqlite.sqlite_retriever.SqliteRetriever"
config = { 
    confidence_threshold = 0.5,
    relevance_threshold = 0.5,
    max_results = 5,
    return_results = 3,
    db_path = "./sqlite_db"
}

[[tool.orbit.adapters]]
type = "retriever"
datasource = "chroma"
adapter = "qa"
implementation = "retrievers.implementations.chroma.chroma_retriever.ChromaRetriever"
config = {
    confidence_threshold = 0.85,
    relevance_threshold = 0.7,
    embedding_provider = null,
    max_results = 5,
    return_results = 3,
    db_path = "./chroma_db"
}

[tool.orbit.datasources]
chroma = {
    use_local = true,
    db_path = "./chroma_db",
    host = "localhost",
    port = 8000,
    confidence_threshold = 0.85,
    relevance_threshold = 0.7,
    embedding_provider = null
}

sqlite = {
    db_path = "sqlite_db",
    confidence_threshold = 0.5,
    relevance_threshold = 0.5,
    max_results = 10,
    return_results = 3
}

postgres = {
    host = "localhost",
    port = 5432,
    database = "retrieval",
    username = "${DATASOURCE_POSTGRES_USERNAME}",
    password = "${DATASOURCE_POSTGRES_PASSWORD}",
    confidence_threshold = 0.5,
    relevance_threshold = 0.5,
    max_results = 10,
    return_results = 3
}

milvus = {
    host = "localhost",
    port = 19530,
    dim = 768,
    metric_type = "IP",
    embedding_provider = null
}

pinecone = {
    api_key = "${DATASOURCE_PINECONE_API_KEY}",
    environment = "${DATASOURCE_PINECONE_ENVIRONMENT}",
    index_name = "${DATASOURCE_PINECONE_INDEX_NAME}",
    embedding_provider = null
}

elasticsearch = {
    node = "https://localhost:9200",
    auth = {
        username = "${DATASOURCE_ELASTICSEARCH_USERNAME}",
        password = "${DATASOURCE_ELASTICSEARCH_PASSWORD}"
    },
    embedding_provider = null
}

mongodb = {
    host = "localhost",
    port = 27017,
    database = "orbit",
    apikey_collection = "api_keys",
    username = "${DATASOURCE_MONGODB_USERNAME}",
    password = "${DATASOURCE_MONGODB_PASSWORD}"
}

[tool.orbit.inference]
ollama = {
    base_url = "http://localhost:11434",
    temperature = 0.1,
    top_p = 0.8,
    top_k = 20,
    repeat_penalty = 1.1,
    num_predict = 1024,
    num_ctx = 8192,
    num_threads = 8,
    model = "gemma3:1b",
    embed_model = "nomic-embed-text",
    stream = true
}

vllm = {
    host = "3.96.164.110",
    port = 5000,
    temperature = 0.1,
    top_p = 0.8,
    top_k = 20,
    max_tokens = 1024,
    model = "Qwen2.5-14B-Instruct",
    stream = true
}

llama_cpp = {
    model_path = "gguf/gemma-3-4b-it-q4_0.gguf",
    chat_format = "chatml",
    temperature = 0.1,
    top_p = 0.8,
    top_k = 20,
    max_tokens = 1024,
    n_ctx = 1024,
    n_threads = 4,
    stream = true,
    n_gpu_layers = -1,
    main_gpu = 0,
    tensor_split = null
}

gemini = {
    api_key = "${GOOGLE_API_KEY}",
    model = "gemini-2.0-flash",
    temperature = 0.1,
    top_p = 0.8,
    top_k = 20,
    max_tokens = 1024,
    stream = true
}

groq = {
    api_key = "${GROQ_API_KEY}",
    model = "llama3-8b-8192",
    temperature = 0.1,
    top_p = 0.8,
    max_tokens = 1024,
    stream = true
}

deepseek = {
    api_key = "${DEEPSEEK_API_KEY}",
    api_base = "https://api.deepseek.com/v1",
    model = "deepseek-chat",
    temperature = 0.1,
    top_p = 0.8,
    max_tokens = 1024,
    stream = true
}

vertex = {
    project_id = "${GOOGLE_CLOUD_PROJECT}",
    location = "us-central1",
    model = "gemini-1.5-pro",
    temperature = 0.1,
    top_p = 0.8,
    top_k = 20,
    max_tokens = 1024,
    credentials_path = "",
    stream = true
}

aws = {
    access_key = "${AWS_BEDROCK_ACCESS_KEY}",
    secret_access_key = "${AWS_SECRET_ACCESS_KEY}",
    region = "ca-central-1",
    model = "anthropic.claude-3-sonnet-20240229-v1:0"
}

azure = {
    base_url = "http://azure-ai.endpoint.microsoft.com",
    deployment = "azure-ai-deployment",
    api_key = "${AZURE_ACCESS_KEY}"
}

openai = {
    api_key = "${OPENAI_API_KEY}",
    model = "gpt-4.1",
    temperature = 0.1,
    top_p = 0.8,
    max_tokens = 1024,
    stream = true
}

mistral = {
    api_key = "${MISTRAL_API_KEY}",
    api_base = "https://api.mistral.ai/v1",
    model = "mistral-small-latest",
    temperature = 0.1,
    top_p = 0.8,
    max_tokens = 1024,
    stream = true
}

anthropic = {
    api_key = "${ANTHROPIC_API_KEY}",
    api_base = "https://api.anthropic.com/v1",
    model = "claude-3.5-sonnet",
    temperature = 0.1,
    top_p = 0.8,
    max_tokens = 1024,
    stream = true
}

[tool.orbit.safety]
enabled = false
mode = "fuzzy"
moderator = "openai"
max_retries = 3
retry_delay = 1.0
request_timeout = 10
allow_on_timeout = false

[tool.orbit.moderators]
openai = {
    api_key = "${OPENAI_API_KEY}",
    model = "omni-moderation-latest"
}

anthropic = {
    api_key = "${ANTHROPIC_API_KEY}",
    model = "claude-3-haiku-20240307",
    temperature = 0.0,
    max_tokens = 10,
    batch_size = 5
}

ollama = {
    base_url = "http://localhost:11434",
    model = "granite3.3:2b",
    temperature = 0.0,
    top_p = 1.0,
    max_tokens = 50,
    batch_size = 1
}

[tool.orbit.reranker]
enabled = false
provider_override = null
model = "gemma3:1b"
batch_size = 5
temperature = 0.0
top_n = 3

[tool.orbit.rerankers]
cohere = {
    api_key = "${COHERE_API_KEY}",
    model = "rerank-english-v3.0",
    top_n = 5,
    batch_size = 32
}

openai = {
    api_key = "${OPENAI_API_KEY}",
    model = "gpt-4o",
    temperature = 0.0,
    max_tokens = 512,
    batch_size = 20
}

anthropic = {
    api_key = "${ANTHROPIC_API_KEY}",
    model = "claude-3-haiku-20240307",
    temperature = 0.0,
    max_tokens = 512,
    batch_size = 10
}

ollama = {
    base_url = "http://localhost:11434",
    model = "gemma3:1b",
    temperature = 0.0,
    batch_size = 5
}

huggingface = {
    model = "BAAI/bge-reranker-large",
    device = "cpu",
    batch_size = 16
}

jina = {
    api_key = "${JINA_API_KEY}",
    model = "jina-reranker-v2-base-en",
    batch_size = 20
}

vertex = {
    project_id = "${GOOGLE_CLOUD_PROJECT}",
    location = "us-central1",
    model = "text-bison@002",
    temperature = 0.0,
    max_tokens = 256,
    batch_size = 8,
    credentials_path = ""
} 