general:
  port: 3000
  verbose: true
  https:
    enabled: false
    port: 3443
    cert_file: "./cert.pem"
    key_file: "./key.pem"
  session_id:
    header_name: "X-Session-ID"
    required: true
  inference_provider: "llama_cpp"
  language_detection: false
  inference_only: true
  adapter: "qa-sql"

messages:
  no_results_response: "I'm sorry, but I don't have any specific information about that topic in my knowledge base."
  collection_not_found: "I couldn't find the requested collection. Please make sure the collection exists before querying it."

embedding:
  provider: "ollama"
  enabled: false

api_keys:
  header_name: "X-API-Key"
  prefix: "orbit_"

logging:
  level: "INFO"
  handlers:
    file:
      enabled: true
      directory: "logs"
      filename: "orbit.log"
      max_size_mb: 10
      backup_count: 30
      rotation: "midnight"
      format: "text"
    console:
      enabled: false
      format: "text"
  capture_warnings: true
  propagate: false
  loggers:
    inference.clients.llama_cpp:
      level: "ERROR"
    llama_cpp:
      level: "ERROR"
    llama_cpp.llama:
      level: "ERROR"
    ggml:
      level: "ERROR"
    metal:
      level: "ERROR"

internal_services:
  elasticsearch:
    enabled: false
    node: ${INTERNAL_SERVICES_ELASTICSEARCH_NODE}
    index: 'orbit'
    username: ${INTERNAL_SERVICES_ELASTICSEARCH_USERNAME}
    password: ${INTERNAL_SERVICES_ELASTICSEARCH_PASSWORD}

  mongodb:
    host: ${INTERNAL_SERVICES_MONGODB_HOST}
    port: ${INTERNAL_SERVICES_MONGODB_PORT}
    database: "orbit"
    apikey_collection: "api_keys"
    username: ${INTERNAL_SERVICES_MONGODB_USERNAME}
    password: ${INTERNAL_SERVICES_MONGODB_PASSWORD}

  redis:
    enabled: false
    host: ${INTERNAL_SERVICES_REDIS_HOST}
    port: ${INTERNAL_SERVICES_REDIS_PORT}
    db: 0
    username: ${INTERNAL_SERVICES_REDIS_USERNAME}
    password: ${INTERNAL_SERVICES_REDIS_PASSWORD}
    use_ssl: false
    ttl: 604800

# This feature requires a MongoDB instance
chat_history:
  enabled: false
  collection_name: "chat_history"
  default_limit: 20
  store_metadata: true
  retention_days: 1
  max_tracked_sessions: 10000
  session:
    required: true
    header_name: "X-Session-ID"
  user:
    header_name: "X-User-ID"
    required: false

embeddings:
  llama_cpp:
    model_path: "gguf/nomic-embed-text-v1.5-Q4_0.gguf"
    model: "nomic-embed-text-v1.5-Q4_0"
    n_ctx: 1024 
    n_threads: 4
    n_gpu_layers: 0
    main_gpu: 0 
    tensor_split: null  # Optional: GPU memory split for multi-GPU setups
    batch_size: 8
    dimensions: 768
    embed_type: "llama_embedding"
  ollama:
    base_url: "http://localhost:11434"
    model: "nomic-embed-text"
    dimensions: 768
  jina:
    api_key: ${JINA_API_KEY}
    base_url: "https://api.jina.ai/v1"
    model: "jina-embeddings-v3"
    task: "text-matching"
    dimensions: 1024
    batch_size: 10
  openai:
    api_key: ${OPENAI_API_KEY}
    model: "text-embedding-3-large"
    dimensions: 1024
    batch_size: 10
  cohere:
    api_key: ${COHERE_API_KEY}
    model: "embed-english-v3.0"
    input_type: "search_document"
    dimensions: 1024
    batch_size: 32
    truncate: "NONE"
    embedding_types: ["float"]
  mistral:
    api_key: ${MISTRAL_API_KEY}
    api_base: "https://api.mistral.ai/v1"
    model: "mistral-embed"
    dimensions: 1024

# This feature requires a MongoDB instance
adapters:
  - name: "qa-sql"
    type: "retriever"
    datasource: "sqlite"
    adapter: "qa"
    implementation: "retrievers.implementations.qa_sql_retriever.QASSQLRetriever"
    config:
      confidence_threshold: 0.3
      max_results: 5
      return_results: 3

  - name: "qa-vector"
    type: "retriever"
    datasource: "chroma"
    adapter: "qa"
    implementation: "retrievers.implementations.qa_chroma_retriever.QAChromaRetriever"
    config:
      confidence_threshold: 0.3
      distance_scaling_factor: 200.0
      embedding_provider: null
      max_results: 5
      return_results: 3

datasources:
  chroma:
    use_local: true
    db_path: "sample_db/chroma/chroma_db"
    host: "localhost"
    port: 8000
    embedding_provider: null 
  sqlite:
    db_path: "sample_db/sqlite/sqlite_db"
  postgres:
    host: "localhost"
    port: 5432
    database: "retrieval"
    username: ${DATASOURCE_POSTGRES_USERNAME}
    password: ${DATASOURCE_POSTGRES_PASSWORD}
  milvus:
    host: "localhost"
    port: 19530
    dim: 768
    metric_type: "IP"  # Options: L2, IP, COSINE
    embedding_provider: null
  pinecone:
    api_key: ${DATASOURCE_PINECONE_API_KEY}
    environment: ${DATASOURCE_PINECONE_ENVIRONMENT}
    index_name: ${DATASOURCE_PINECONE_INDEX_NAME}
    embedding_provider: null
  elasticsearch:
    node: 'https://localhost:9200'
    auth:
      username: ${DATASOURCE_ELASTICSEARCH_USERNAME}
      password: ${DATASOURCE_ELASTICSEARCH_PASSWORD}
    embedding_provider: null
  mongodb:
    host: "localhost"
    port: 27017
    database: "orbit"
    apikey_collection: "api_keys"
    username: ${DATASOURCE_MONGODB_USERNAME}
    password: ${DATASOURCE_MONGODB_PASSWORD}

inference:
  ollama:
    base_url: "http://localhost:11434"
    temperature: 0.1
    top_p: 0.8
    top_k: 20
    repeat_penalty: 1.1
    num_predict: 1024
    num_ctx: 8192
    num_threads: 8
    model: "gemma3:1b"
    stream: true
  vllm:
    host: "localhost"
    port: 5000
    temperature: 0.1
    top_p: 0.8
    top_k: 20
    max_tokens: 1024
    model: "Qwen2.5-14B"
    stream: true
  llama_cpp:
    model_path: "gguf/tinyllama-1.1b-chat-v1.0.Q4_0.gguf"
    chat_format: "chatml"  # Chat format to use (chatml, llama-2, gemma, etc.)
    verbose: false
    temperature: 0.1
    top_p: 0.8
    top_k: 20
    max_tokens: 1024
    repeat_penalty: 1.1
    n_ctx: 1024
    n_threads: 4
    stream: true
    n_gpu_layers: 0  # For GPU/Metal support
    main_gpu: 0
    tensor_split: null
    stop_tokens: [
      "<|im_start|>", 
      "<|im_end|>",
      "<|endoftext|>"
    ]
  gemini:
    api_key: ${GOOGLE_API_KEY}
    model: "gemini-2.0-flash"
    temperature: 0.1
    top_p: 0.8
    top_k: 20
    max_tokens: 1024
    stream: true
  groq:
    api_key: ${GROQ_API_KEY}
    model: "llama3-8b-8192"
    temperature: 0.1
    top_p: 0.8
    max_tokens: 1024
    stream: true
  deepseek:
    api_key: ${DEEPSEEK_API_KEY}
    api_base: "https://api.deepseek.com/v1"
    model: "deepseek-chat"
    temperature: 0.1
    top_p: 0.8
    max_tokens: 1024
    stream: true
  vertex:
    project_id: ${GOOGLE_CLOUD_PROJECT}
    location: "us-central1"
    model: "gemini-1.5-pro"
    temperature: 0.1
    top_p: 0.8
    top_k: 20
    max_tokens: 1024
    credentials_path: ""
    stream: true
  aws:
    access_key: ${AWS_BEDROCK_ACCESS_KEY}
    secret_access_key: ${AWS_SECRET_ACCESS_KEY}
    region: "ca-central-1"
    model: "anthropic.claude-3-sonnet-20240229-v1:0"
    content_type: "application/json"
    accept: "application/json"
    max_tokens: 1024
  azure:
    base_url: http://azure-ai.endpoint.microsoft.com
    deployment: "azure-ai-deployment"
    api_key: ${AZURE_ACCESS_KEY}
    temperature: 0.1
    top_p: 0.8
    max_tokens: 1024
    stream: true
    verbose: true
  openai:
    api_key: ${OPENAI_API_KEY}
    model: "gpt-4.1"
    temperature: 0.1
    top_p: 0.8
    max_tokens: 1024
    stream: true
  mistral:
    api_key: ${MISTRAL_API_KEY}
    api_base: "https://api.mistral.ai/v1"
    model: "mistral-small-latest"
    temperature: 0.1
    top_p: 0.8
    max_tokens: 1024
    stream: true
  anthropic:
    api_key: ${ANTHROPIC_API_KEY}
    api_base: "https://api.anthropic.com/v1"
    model: "claude-sonnet-4-20250514"
    temperature: 0.1
    top_p: 0.8
    max_tokens: 1024
    stream: true
  together:
    api_key: ${TOGETHER_API_KEY}
    api_base: "https://api.together.xyz/v1"
    model: "Qwen/Qwen3-235B-A22B-fp8-tput"
    temperature: 0.1
    top_p: 0.8
    max_tokens: 1024
    stream: true
    show_thinking: false
  xai:
    api_key: ${XAI_API_KEY}
    api_base: "https://api.x.ai/v1"
    model: "grok-3-mini-beta"
    temperature: 0.1
    top_p: 0.8
    max_tokens: 1024
    stream: true
    show_thinking: false
  huggingface:
    model_name: "HuggingFaceTB/SmolLM2-1.7B-Instruct"
    device: "cpu"
    max_length: 1024
    temperature: 0.7
    top_p: 0.9
    stream: false,
  openrouter:
    api_key: ${OPENROUTER_API_KEY}
    base_url: "https://openrouter.ai/api/v1"
    model: "openai/gpt-4o"
    temperature: 0.1
    top_p: 0.8
    max_tokens: 1024
    stream: true
    verbose: false

safety:
  enabled: false
  mode: "fuzzy"
  moderator: "ollama"
  max_retries: 3
  retry_delay: 1.0
  request_timeout: 10
  allow_on_timeout: false
  safety_prompt_path: "prompts/safety_prompt.txt"

moderators:
  openai:
    api_key: ${OPENAI_API_KEY}
    model: "omni-moderation-latest"
  anthropic:
    api_key: ${ANTHROPIC_API_KEY}
    model: "claude-3-haiku-20240307"
    temperature: 0.0
    max_tokens: 10
    batch_size: 5
  ollama:
    base_url: "http://localhost:11434"
    model: "granite3.3:2b"
    temperature: 0.0
    top_p: 1.0
    max_tokens: 50
    batch_size: 1

reranker:
  provider: "ollama"
  enabled: false

rerankers:
  ollama:
    base_url: "http://localhost:11434"
    model: "xitao/bge-reranker-v2-m3:latest"
    temperature: 0.0
    batch_size: 5