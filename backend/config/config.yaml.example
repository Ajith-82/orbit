general:
  port: 3000
  verbose: "false"
  https:
    enabled: false
    port: 3443
    cert_file: "./cert.pem"
    key_file: "./key.pem"

safety:
  mode: "fuzzy"  # Options: strict, fuzzy, disabled
  model: "gemma3:12b"
  max_retries: 3
  retry_delay: 1.0
  request_timeout: 15
  allow_on_timeout: false  # Set to true to allow queries if safety check times out
  temperature: 0.0  # Use 0 for deterministic response
  top_p: 1.0
  top_k: 1
  num_predict: 20  # Limit response length for safety checks
  stream: false
  repeat_penalty: 1.1

chroma:
  host: "localhost"
  port: 8000
  collection: "occsc"
  confidence_threshold: 0.7
  relevance_threshold: 0.5

elasticsearch:
  enabled: false
  node: 'localhost:9200'
  index: 'chatbot'
  auth:
    username: ${ELASTICSEARCH_USERNAME}
    password: ${ELASTICSEARCH_PASSWORD}

ollama:
  base_url: "http://localhost:11434"
  temperature: 0.01
  top_p: 0.8
  top_k: 20
  repeat_penalty: 1.2
  num_predict: 1024
  num_ctx: 8192
  num_threads: 8
  model: "chatbot"
  embed_model: "nomic-embed-text"
  max_response_length: 200
  stream: true

vllm:
  base_url: "http://localhost:5000"
  temperature: 0.01
  max_tokens: 32
  model: "VLLMQwen2.5-14B"
  top_p: 0.8
  frequency_penalty: 0.0
  presence_penalty: 0.0
  best_of: 1
  n: 1
  logprobs: null
  echo: false
  stream: false