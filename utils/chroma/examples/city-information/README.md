# City Information Retrieval System

This is a city QA service by using rich metadata for LLM-powered answers and analytical insights. The ORBIT inference client uses metadata to generate factual answers while also performing calculations, trend analysis, and comparisons to provide more valuable and data-driven responses.

## Overview

The City Insights System adds the following capabilities:

1. **Rich Metadata** - QA pairs include metadata like costs, response times, departments, statistics, etc.
2. **LLM-generated Answers** - Answers are dynamically generated by the LLM based on the metadata
3. **Analytical Reasoning** - LLMs analyze the metadata to identify patterns, perform calculations, and draw insights
4. **Contextual Enhancement** - Responses combine factual answers with data-driven insights

## Components

The system consists of the following components:

- **Metadata-Rich QA Data** - JSON data with metadata fields but no pre-written answers
- **Collection Creation Script** - Preserves all metadata in Chroma
- **Insights Engine** - Service that uses metadata to generate answers and insights
- **Insights Adapter** - Connects the QA system with the insights engine

## Setup Instructions

### 1. Create the Enhanced Collection

First, create a Chroma collection with the metadata-rich QA data:

```bash
python utils/chroma/scripts/create_enhanced_qa_collection.py city_qa_enhanced utils/sample-data/city-qa-pairs-enhanced.json
```

For local development mode:

```bash
python utils/chroma/scripts/create_enhanced_qa_collection.py city_qa_enhanced utils/sample-data/city-qa-pairs-enhanced.json --local
```

### 2. Run the Demo

Run the demo script to see the LLM-generated answers and insights in action:

```bash
python examples/city_insights_demo.py
```

For local development mode:

```bash
python examples/city_insights_demo.py --local
```

## Metadata Fields

The system uses a variety of metadata fields to store information:

| Field | Description | Example Value |
|-------|-------------|---------------|
| department | Department responsible for service | "Public Works" |
| response_time | Time to respond/process (days) | 3 |
| cost | Cost to citizen | "$25" |
| timeframe | Frequency or seasonal info | "annual" |
| location | Physical location | "City Hall" |
| priority | Service priority level | "high" |
| online_completion_rate | Online usage percentage | 0.78 |
| processing_volume | Processing volume | 120 |
| annual_revenue | Revenue generated | "$12750" |
| approval_rate | Application approval rate | 0.92 |

You can add custom metadata fields for specific use cases.

## Example Insights

The system can generate insights like:

1. **Cost Comparisons**:
   "The parking permit fee ($25) is 87% less than the average business license fee ($192)."

2. **Efficiency Analysis**:
   "The Transportation Department responds to traffic signal issues within 0.5 days on average, making it 83% faster than the citywide average response time of 3 days."

3. **Revenue Impacts**:
   "While the garage sale permit fee seems small at $5, it generates approximately $4,750 in annual revenue from 950 permits, representing 6.9% of the City Clerk's office revenue."

4. **Usage Analytics**:
   "82% of park picnic area reservations are completed online, saving approximately 273 staff hours annually compared to in-person reservations."

5. **Seasonal Patterns**:
   "Public Works handles 3x more service requests during fall leaf collection season than winter months."

## Benefits of LLM-generated Answers

Having the LLM generate answers directly from metadata provides several advantages:

1. **Up-to-date Accuracy** - Answers always reflect the latest metadata
2. **Content Dynamism** - Answers can be tailored to different user needs
3. **Consistency** - The answer style and content matches the accompanying insights
4. **Maintenance** - Only the metadata needs to be updated, not pre-written answers
5. **Personalization** - Answers can be personalized based on user context or preferences

## Extending the System

To add new capabilities:

1. Add relevant metadata fields to the QA pairs JSON file
2. Update the insights engine prompting to take advantage of the new fields
3. Modify the insights adapter if needed to extract and present the insights

## Technical Details

The system uses ChromaDB for vector storage and retrieval, with all metadata stored alongside the embeddings. The insights engine uses the LLM service to analyze the metadata and generate both answers and insights.

The embedding focuses only on the questions, which means searches retrieve the right QA pairs while bringing along all the rich metadata needed for analysis and answer generation.

## Additional Resources

- `utils/insights_engine.py` - Core logic for generating answers and insights
- `utils/insights_adapter.py` - Integration with QA system
- `examples/city_insights_demo.py` - Demonstration of capabilities 