# Orbit Server Dependencies Configuration
# This file defines dependency profiles for different use cases

[profiles.minimal]
description = "Core dependencies for basic Ollama operation"
dependencies = [
    "ollama==0.4.8",
    "motor>=3.7.0",
    "pymongo>=4.12.0",
    "redis>=6.1.0",
    "chromadb>=1.0.9",
    "langchain-ollama>=0.2.3",
    "langchain-community>=0.0.10",
    "pydantic>=2.10.0",
    "PyYAML>=6.0.1",
    "fastapi>=0.115.9",
    "uvicorn==0.34.2",
    "python-dotenv==1.0.1",
    "requests==2.31.0",
    "python-multipart>=0.0.14",
    "langid==1.1.6",
    "pycld2==0.42",
    "langdetect>=1.0.9",
    "pytest>=8.3.5",
    "python-json-logger>=2.0.7",
    "tqdm>=4.66.2",
    "aiohttp>=3.11.1",
    "aiodns>=3.2.0",
    "regex==2024.11.6",
    "sseclient-py==1.8.0",
    "psutil==6.0.0",
    "pycountry>=24.6.1",
    "llama-cpp-python==0.3.9",
    "elasticsearch==9.0.0",
]

[profiles.huggingface]
description = "Dependencies for Hugging Face model support"
extends = "minimal"
dependencies = [
    "huggingface-hub==0.30.2",
    "safetensors==0.5.3",
    "torch==2.1.0",  # Note: pytorch package name in pip is 'torch'
    "transformers==4.35.0",
]

[profiles.commercial]
description = "Dependencies for commercial/cloud inference providers"
extends = "minimal"
dependencies = [
    "openai==1.76",
    "anthropic==0.50.0",
    "google-generativeai==0.8.5",
    "cohere==5.15.0",
    "groq==0.23.1",
    "deepseek==1.0.0",
    "mistralai==1.7.0",
    "together==1.5.7",
    "boto3==1.38.13",
    "azure-ai-inference==1.0.0b9",
]

[profiles.all]
description = "All available dependencies"
extends = ["minimal", "huggingface", "commercial"]
dependencies = []

[profiles.development]
description = "Development and testing dependencies"
extends = "minimal"
dependencies = [
    "pytest>=8.3.5",
    "pytest-asyncio>=0.26.0",
    "pytest-cov>=6.0.0",
    "black>=24.10.0",
    "flake8>=7.1.1",
    "mypy>=1.13.0",
    "pre-commit>=4.0.1",
]

# GGUF model configuration
[models.gguf.gemma_3_1b]
url = "https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q4_0.gguf"
path = "server/gguf/gemma-3-1b-it-Q4_0.gguf"
description = "Gemma 3 1B quantized model"

# Custom profile example - users can add their own
[profiles.custom_example]
description = "Example custom profile with specific providers"
extends = "minimal"
dependencies = [
    "openai==1.76",
    "anthropic==0.50.0",
    # Add only the providers you need
]