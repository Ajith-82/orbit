general:
  port: 3001
  verbose: true
  https:
    enabled: false
    port: 3443
    cert_file: "./cert.pem"
    key_file: "./key.pem"
  inference_provider: "ollama"
  datasource_provider: "sqlite"

messages:
  no_results_response: "I'm sorry, but I don't have any specific information about that topic in my knowledge base."
  collection_not_found: "I couldn't find the requested collection. Please make sure the collection exists before querying it."

embedding:
  provider: "ollama"
  enabled: false
  fail_on_error: false

api_keys:
  header_name: "X-API-Key"
  require_for_health: true

logging:
  level: "INFO"
  file:
    enabled: true
    directory: "logs"
    filename: "server.log"
    max_size_mb: 10
    backup_count: 30
    rotation: "midnight"  # Options: midnight, h (hourly), d (daily)
    format: "json"  # Options: json, text
  console:
    enabled: true
    format: "text"  # Options: json, text
  capture_warnings: true
  propagate: false

internal_services:
  elasticsearch:
    enabled: false
    node: 'https://localhost:9200'
    index: 'orbit'
    api_key: ${INTERNAL_SERVICES_ELASTICSEARCH_API_KEY}

  mongodb:
    host: "localhost"
    port: 27017
    database: "orbit"
    apikey_collection: "api_keys"
    username: ${INTERNAL_SERVICES_MONGODB_USERNAME}
    password: ${INTERNAL_SERVICES_MONGODB_PASSWORD}

embeddings:
  ollama:
    base_url: "http://localhost:11434"
    model: "nomic-embed-text"
  huggingface:
    model: "sentence-transformers/all-mpnet-base-v2"
    device: "cpu"  # Options: cpu, cuda
    normalize: true
  openai:
    api_key: ${OPENAI_API_KEY}
    model: "text-embedding-3-small"
    dimensions: 1536
    batch_size: 10
  bedrock:
    region: "ca-central-1"
    model: "amazon.titan-embed-text-v1"
    dimensions: 1536
  cohere:
    api_key: ${COHERE_API_KEY}
    model: "embed-english-v3.0"
    truncation: true
  mistral:
    api_key: ${MISTRAL_API_KEY}
    api_base: "https://api.mistral.ai/v1"
    model: "mistral-embed"
    dimensions: 1024

datasources:
  chroma:
    use_local: true
    db_path: "../utils/chroma/chroma_db"
    host: "localhost"
    port: 8000
    domain_adapter: "qa"
    confidence_threshold: 0.85
    relevance_threshold: 0.7
    embedding_provider: null
  sqlite:
    db_path: "./rag_database.db"
    domain_adapter: "qa"
    confidence_threshold: 0.5
    relevance_threshold: 0.5
    max_results: 10
    return_results: 3
    adapter_params:
      boost_exact_matches: true
  postgres:
    host: "localhost"
    port: 5432
    username: ${DATASOURCE_POSTGRES_USERNAME}
    password: ${DATASOURCE_POSTGRES_PASSWORD}
    domain_adapter: "qa"
    confidence_threshold: 0.6
    relevance_threshold: 0.5
    max_results: 10
    return_results: 3
    adapter_params:
      boost_exact_matches: true
  milvus:
    host: "localhost"
    port: 19530
    dim: 768
    metric_type: "IP"  # Options: L2, IP, COSINE
    embedding_provider: null
  pinecone:
    api_key: ${DATASOURCE_PINECONE_API_KEY}
    environment: ${DATASOURCE_PINECONE_ENVIRONMENT}
    index_name: ${DATASOURCE_PINECONE_INDEX_NAME}
    embedding_provider: null
  elasticsearch:
    node: 'https://localhost:9200'
    auth:
      username: ${DATASOURCE_ELASTICSEARCH_USERNAME}
      password: ${DATASOURCE_ELASTICSEARCH_PASSWORD}
    embedding_provider: null
  mongodb:
    host: "localhost"
    port: 27017
    database: "orbit"
    apikey_collection: "api_keys"
    username: ${DATASOURCE_MONGODB_USERNAME}
    password: ${DATASOURCE_MONGODB_PASSWORD}    

# Inference providers
inference:
  ollama:
    base_url: "http://localhost:11434"
    temperature: 0.1
    top_p: 0.8
    top_k: 20
    repeat_penalty: 1.1
    num_predict: 1024
    num_ctx: 8192
    num_threads: 8
    model: "gemma3:1b"
    stream: true
  vllm:
    host: "3.96.164.110"
    port: 5000
    temperature: 0.1
    top_p: 0.8
    top_k: 20
    max_tokens: 1024
    model: "Qwen2.5-14B-Instruct"
    stream: true
  llama_cpp:
    model_path: "gguf/gemma-3-4b-it-q4_0.gguf" # Use script download_hugging_face_gguf_model.py to get GGUF files from huggingface
    chat_format: "chatml"  # Chat format to use (chatml, llama-2, gemma, etc.)
    temperature: 0.1
    top_p: 0.8
    top_k: 20
    max_tokens: 1024
    n_ctx: 4096
    n_threads: 4
    stream: true
    # GPU acceleration parameters
    n_gpu_layers: -1  # -1 means offload all layers to GPU
    main_gpu: 0       # Use the first GPU (index 0)
    tensor_split: null # Optional: distribute model across multiple GPUs e.g. [0.5, 0.5]
  gemini:
    api_key: ${GOOGLE_API_KEY}
    model: "gemini-2.0-flash"
    temperature: 0.1
    top_p: 0.8
    top_k: 20
    max_tokens: 1024
    stream: true
  groq:
    api_key: ${GROQ_API_KEY}
    model: "llama3-8b-8192"
    temperature: 0.1
    top_p: 0.8
    max_tokens: 1024
    stream: true
  deepseek:
    api_key: ${DEEPSEEK_API_KEY}
    api_base: "https://api.deepseek.com/v1"
    model: "deepseek-chat"
    temperature: 0.1
    top_p: 0.8
    max_tokens: 1024
    stream: true
  vertex:
    project_id: ${GOOGLE_CLOUD_PROJECT}
    location: "us-central1"
    model: "gemini-1.5-pro"
    temperature: 0.1
    top_p: 0.8
    top_k: 20
    max_tokens: 1024
    credentials_path: ""  # Path to service account key file, if not using default credentials
    stream: true
  aws:
    access_key: ${AWS_BEDROCK_ACCESS_KEY}
    secret_access_key: ${AWS_SECRET_ACCESS_KEY}
    region: "ca-central-1"
    model: "anthropic.claude-3-sonnet-20240229-v1:0"
  azure:
    base_url: http://azure-ai.endpoint.microsoft.com
    deployment: "azure-ai-deployment"
    api_key: ${AZURE_ACCESS_KEY}
  openai:
    api_key: ${OPENAI_API_KEY}
    model: "gpt-4.1"
    temperature: 0.1
    top_p: 0.8
    max_tokens: 1024
    stream: true
  mistral:
    api_key: ${MISTRAL_API_KEY}
    api_base: "https://api.mistral.ai/v1"
    model: "mistral-small-latest"
    temperature: 0.1
    top_p: 0.8
    max_tokens: 1024
    stream: true
  anthropic:
    api_key: ${ANTHROPIC_API_KEY}
    api_base: "https://api.anthropic.com/v1"
    model: "claude-3.5-sonnet"
    temperature: 0.1
    top_p: 0.8
    max_tokens: 1024
    stream: true 
  pytorch:
      model_path: "facebook/opt-125m"  # Path to HF model or local model directory
      system_prompt: "You are a helpful, accurate, precise, and expert assistant."
      trust_remote_code: true
      
      # Device settings
      use_mps: true  # Enable MPS for Apple Silicon
      
      # Tokenizer settings
      use_fast_tokenizer: true
      padding_side: "left"
      truncation_side: "left"
      
      # Performance optimizations
      precompile_prefill: true
      load_in_8bit: false
      load_in_4bit: false
      int8_threshold: 6.0
      int8_has_fp16_weight: true
      quant_type: "nf4"
      use_double_quant: true
      compute_dtype: "float16"
      
      # Generation settings
      max_batch_size: 1
      do_sample: true
      temperature: 0.7
      top_p: 0.9
      top_k: 40
      max_tokens: 1024
      use_cache: true
      repetition_penalty: 1.1
      cache_size: 100
      stream: true 

safety:
  enabled: false
  mode: "fuzzy"
  provider_override: "ollama" # If null, uses general.inference_provider
  model: "gemma3:1b"
  max_retries: 3
  retry_delay: 1.0
  request_timeout: 10  # Shorter timeout for moderation
  allow_on_timeout: false
  temperature: 0.0
  top_p: 1.0
  top_k: 1
  num_predict: 20
  stream: false
  repeat_penalty: 1.1

reranker:
  enabled: false
  provider_override: null  # If null, uses general.inference_provider
  model: "gemma3:1b"
  batch_size: 5
  temperature: 0.0
  top_n: 3

rerankers:
  cohere:
    api_key: ${COHERE_API_KEY}
    model: "rerank-english-v3.0"
    top_n: 5
    batch_size: 32
  openai:
    api_key: ${OPENAI_API_KEY}
    model: "gpt-4o"
    temperature: 0.0
    max_tokens: 512
    batch_size: 20
  anthropic:
    api_key: ${ANTHROPIC_API_KEY}
    model: "claude-3-haiku-20240307"
    temperature: 0.0
    max_tokens: 512
    batch_size: 10
  ollama:
    base_url: "http://localhost:11434"
    model: "gemma3:1b"
    temperature: 0.0
    batch_size: 5
  huggingface:
    model: "BAAI/bge-reranker-large"
    device: "cpu"  # Options: cpu, cuda
    batch_size: 16
  jina:
    api_key: ${JINA_API_KEY}
    model: "jina-reranker-v2-base-en"
    batch_size: 20
  vertex:
    project_id: ${GOOGLE_CLOUD_PROJECT}
    location: "us-central1"
    model: "text-bison@002"
    temperature: 0.0
    max_tokens: 256
    batch_size: 8
    credentials_path: ""  # Path to service account key file
