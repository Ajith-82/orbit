general:
  port: 3000
  verbose: true
  https:
    enabled: false
    port: 3443
    cert_file: "./cert.pem"
    key_file: "./key.pem"
  inference_provider: "ollama"
  datasource_provider: "chroma"

embedding:
  provider: "ollama"
  enabled: true
  fail_on_error: false

api_keys:
  header_name: "X-API-Key"
  require_for_health: true

logging:
  level: "INFO"  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
  file:
    enabled: true
    directory: "logs"
    filename: "server.log"
    max_size_mb: 10
    backup_count: 30
    rotation: "midnight"  # Options: midnight, h (hourly), d (daily)
    format: "json"  # Options: json, text
  console:
    enabled: true
    format: "text"  # Options: json, text
  capture_warnings: true
  propagate: false

internal_services:
  elasticsearch:
    enabled: true
    node: 'https://localhost:9200'
    index: 'orbit'
    api_key: ${INTERNAL_SERVICES_ELASTICSEARCH_API_KEY}
    
  mongodb:
    host: "localhost"
    port: 27017
    database: "orbit"
    apikey_collection: "api_keys"
    username: ${INTERNAL_SERVICES_MONGODB_USERNAME}
    password: ${INTERNAL_SERVICES_MONGODB_PASSWORD}

embeddings:
  ollama:
    base_url: "http://localhost:11434"
    model: "nomic-embed-text"
  huggingface:
    model: "sentence-transformers/all-mpnet-base-v2"
    device: "cpu"  # Options: cpu, cuda
    normalize: true
  openai:
    api_key: ${OPENAI_API_KEY}
    model: "text-embedding-3-small"
    dimensions: 1536
    batch_size: 10
  bedrock:
    region: "ca-central-1"
    model: "amazon.titan-embed-text-v1"
    dimensions: 1536
  cohere:
    api_key: ${COHERE_API_KEY}
    model: "embed-english-v3.0"
    truncation: true

datasources:
  chroma:
    host: "localhost"
    port: 8000
    confidence_threshold: 0.85
    relevance_threshold: 0.7
    embedding_provider: null
  postgres:
    host: "localhost"
    port: 5432
    database: "retrieval"
    username: ${DATASOURCE_POSTGRES_USERNAME}
    password: ${DATASOURCE_POSTGRES_PASSWORD}
    schema: "public"
    table: "documents"
    embedding_column: "embedding"
    content_column: "content"
    metadata_columns: ["source", "date", "author"]
  milvus:
    host: "localhost"
    port: 19530
    dim: 768
    metric_type: "IP"  # Options: L2, IP, COSINE
    embedding_provider: null
  pinecone:
    api_key: ${DATASOURCE_PINECONE_API_KEY}
    environment: ${DATASOURCE_PINECONE_ENVIRONMENT}
    index_name: ${DATASOURCE_PINECONE_INDEX_NAME}
    embedding_provider: null
  elasticsearch:
    node: 'https://localhost:9200'
    auth:
      username: ${DATASOURCE_ELASTICSEARCH_USERNAME}
      password: ${DATASOURCE_ELASTICSEARCH_PASSWORD}
    embedding_provider: null
  mongodb:
    host: "localhost"
    port: 27017
    database: "orbit"
    apikey_collection: "api_keys"
    username: ${DATASOURCE_MONGODB_USERNAME}
    password: ${DATASOURCE_MONGODB_PASSWORD}    

# Inference providers
inference:
  ollama:
    base_url: "http://localhost:11434"
    temperature: 0.1
    top_p: 0.8
    top_k: 20
    repeat_penalty: 1.1
    num_predict: 1024
    num_ctx: 8192
    num_threads: 8
    model: "gemma3:1b"
    embed_model: "nomic-embed-text"
    stream: true
  vllm:
    host: "http://localhost:5000"
    port: 8000
    temperature: 0.1
    top_p: 0.8
    top_k: 20
    max_tokens: 1024
    model: "llama3:1b"
    embed_model: "nomic-embed-text"
    stream: true
  llama_cpp:
    model_path: "/path/to/models/llama-8b.gguf"
    temperature: 0.1
    top_p: 0.8
    top_k: 20
    max_tokens: 1024
    n_ctx: 8192
    n_threads: 8
    embed_model: "nomic-embed-text"
    stream: true
  bedrock:
    region: "ca-central-1"
    model_id: "anthropic.claude-3-sonnet-20240229-v1:0"
    temperature: 0.1
    top_p: 0.8
    max_tokens: 1024
    embed_model: "amazon.titan-embed-text-v1"
    stream: true
  openai:
    api_key: ${OPENAI_API_KEY}
    model: "gpt-4o"
    temperature: 0.1
    top_p: 0.8
    max_tokens: 1024
    embed_model: "text-embedding-3-large"
    stream: true

safety:
  enabled: false
  mode: "fuzzy"  # Options: strict, fuzzy, disabled
  provider_override: null  # If null, uses general.inference_provider
  model: "gemma3:12b"
  max_retries: 3
  retry_delay: 1.0
  request_timeout: 15
  allow_on_timeout: false
  temperature: 0.0
  top_p: 1.0
  top_k: 1
  num_predict: 20
  stream: false
  repeat_penalty: 1.1

reranker:
  enabled: false
  provider_override: null  # If null, uses general.inference_provider
  model: "gemma3:1b"
  batch_size: 5
  temperature: 0.0
  top_n: 3